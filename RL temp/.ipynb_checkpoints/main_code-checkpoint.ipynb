{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryzoRoA-iuoB"
   },
   "source": [
    "# **Active Object Localization with Deep Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import tensorflow.compat.v1 as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*7))])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\") \n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "yeqbT99oVBC7",
    "outputId": "a196e124-8e60-4682-db7c-21139be74f6e"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download required dataset \n",
    "!kaggle datasets download -d huanghanchina/pascal-voc-2012 -p pascal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xB7qEvokBqgx"
   },
   "source": [
    "**Extracting the zip file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgqLDYH2xcda"
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "os.chdir('./pascal')  #change dir\n",
    "!unzip -q pascal-voc-2012.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./pascal')  #change dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhS-AMuLByO1"
   },
   "source": [
    "**Install and import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "PR8qwxz6yOjs",
    "outputId": "876c76ca-7c97-411a-9c3d-0172f588011d"
   },
   "outputs": [],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHJNJjhYiPK8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import xmltodict\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTaePMB5B3zu"
   },
   "source": [
    "**Prepare and loading images from dataset along with their bounding boxes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjG6qcg1wM75"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_object_name(object_number):\n",
    "    \"\"\"\n",
    "    Converting object's id to object's name\n",
    "    \"\"\"\n",
    "    dictionary={ 1:'aeroplane' , 2:'bicycle', 3:'bird', 4:'boat', \n",
    "                 5:'bottle' , 6:'bus', 7:'car', 8:'cat', 9:'chair', \n",
    "                 10:'cow', 11:'diningtable', 12:'dog' , 13:'horse', \n",
    "                 14:'motorbike', 15:'person', 16:'pottedplant',\n",
    "                 17:'sheep', 18:'sofa', 19:'train', 20:'tvmonitor'}\n",
    "\n",
    "    return dictionary[object_number]\n",
    "\n",
    "def read_image_index(object_name, dataset_path, test):\n",
    "    \"\"\"\n",
    "    Reading the name of images from the txt file of target object\n",
    "    \"\"\"\n",
    "    index_list = []\n",
    "    if test:\n",
    "      index_file_path = dataset_path + \"ImageSets/Main/\" + object_name + \"_trainval.txt\"\n",
    "    else:\n",
    "      index_file_path = dataset_path + \"ImageSets/Main/\" + object_name + \"_train.txt\"\n",
    "    with open(index_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            #only consider those images that consist of the class (aeroplane) we are interested in.\n",
    "            if \"-1\" not in line.split(\" \")[1]:\n",
    "                index_list.append(line.split(\" \")[0])\n",
    "\n",
    "    return index_list\n",
    "\n",
    "def read_image(image_index, dataset_path):\n",
    "    \"\"\"\n",
    "    Loading images using their name from JPEGImages folder\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    image_folder_path = dataset_path + \"JPEGImages/\"\n",
    "    for each_image in image_index:\n",
    "        img = cv2.imread(image_folder_path + each_image + \".jpg\")\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "\n",
    "def load_annotation(image_index, object_name, dataset_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Loading bounding boxes around objects in images\n",
    "    \"\"\"\n",
    "    bounding_box_list = []\n",
    "    annotattion_path = dataset_path + \"Annotations/\"\n",
    "    for each_image in image_index:\n",
    "        path = annotattion_path + each_image + \".xml\"\n",
    "        xml = xmltodict.parse(open(path, 'rb'))\n",
    "        xml_objects = xml['annotation']['object']\n",
    "        if isinstance(xml_objects, list):\n",
    "            for each_object in xml_objects:\n",
    "\n",
    "                if each_object[\"name\"] == object_name:\n",
    "                    xmin = each_object[\"bndbox\"][\"xmin\"]\n",
    "                    ymin = each_object[\"bndbox\"][\"ymin\"]\n",
    "                    xmax = each_object[\"bndbox\"][\"xmax\"]\n",
    "                    ymax = each_object[\"bndbox\"][\"ymax\"]\n",
    "                    bounding_box = (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "                    bounding_box_list.append(bounding_box)\n",
    "                    break\n",
    "        else:\n",
    "            if xml_objects[\"name\"] == object_name:\n",
    "                xmin = xml_objects[\"bndbox\"][\"xmin\"]\n",
    "                ymin = xml_objects[\"bndbox\"][\"ymin\"]\n",
    "                xmax = xml_objects[\"bndbox\"][\"xmax\"]\n",
    "                ymax = xml_objects[\"bndbox\"][\"ymax\"]\n",
    "                bounding_box = (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "                bounding_box_list.append(bounding_box)\n",
    "\n",
    "    return bounding_box_list\n",
    "\n",
    "\n",
    "def load_data(object_number, test):\n",
    "\n",
    "    \"\"\"\n",
    "    Loading dataset images for a specific class by calling corresponding functions\n",
    "    and saving images and their annotations into arrays\n",
    "    \"\"\" \n",
    "    dataset_path = \"./VOC2012/\"\n",
    "    object_name = get_object_name(object_number)  \n",
    "    image_index = read_image_index(object_name, dataset_path,test)\n",
    "    #print(len(image_index))\n",
    "    image_list = np.asarray(read_image(image_index, dataset_path))\n",
    "    bounding_box_list = np.asarray(load_annotation(image_index, object_name, dataset_path))\n",
    "    \n",
    "    if test:\n",
    "      np.save(object_name + \"_valimage.npy\", image_list)\n",
    "      np.save(object_name + \"_valbox.npy\", bounding_box_list)\n",
    "\n",
    "    else:\n",
    "      np.save(object_name + \"_image.npy\", image_list)\n",
    "      np.save(object_name + \"_box.npy\", bounding_box_list)\n",
    "\n",
    "    return image_list, bounding_box_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STknfrr7CBvf"
   },
   "source": [
    "**Showing some pictures from 20 classes of dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "colab_type": "code",
    "id": "amoVpSmoDJFs",
    "outputId": "85764130-76ba-41a3-d23e-9efe7d3543cf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(1,21):\n",
    "    plt.subplot(5,4,i)\n",
    "    image_list, bounding_box_list = load_data(i,test=False)\n",
    "    im = image_list[15]\n",
    "    # Draw a rectangle with blue line borders of thickness of 1 px\n",
    "    mask = cv2.rectangle(im, (bounding_box_list[15][0], bounding_box_list[15][1]), (bounding_box_list[15][2], bounding_box_list[15][3])  , (255, 0, 0) , 3) \n",
    "    masked_data = cv2.bitwise_and(im, im, mask)\n",
    "    plt.imshow(masked_data)\n",
    "    label = get_object_name(i)\n",
    "    plt.title(str(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsjIZeidCI1m"
   },
   "source": [
    "**Showing 10 sample test images (this funciton is called after training the agent and testing it on 100 test images)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7xrLZacUbGv"
   },
   "outputs": [],
   "source": [
    "def test2():\n",
    "    object_number = 1\n",
    "    image_list, bounding_box_list = load_data(object_number ,test=True)\n",
    "    iou = []\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(\"model.h5\")\n",
    "\n",
    "    for i in range(30,40):\n",
    "        bounding_box = bounding_box_list[i]\n",
    "        image = image_list[i]\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        current_mask = np.asarray([0, 0, width, height])\n",
    "        feature = extract_feature(image, history, vgg16)\n",
    "        end = False\n",
    "        masks = []\n",
    "        step = 0\n",
    "        \n",
    "        while not end:\n",
    "\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "\n",
    "            action = np.argmax(q_value)\n",
    "\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            if action == 8 or step == 10: #steps should be changed to 40\n",
    "                end = True\n",
    "                #print(\"end\")\n",
    "                plt.figure()\n",
    "                new_mask = current_mask\n",
    "                cv2.rectangle(image, (int(new_mask[0]), int(new_mask[1])),\n",
    "                              (int(new_mask[2]), int(new_mask[3])), (255, 0, 0), 1)\n",
    "                '''cv2.imwrite(\"./result/plane_result%d.jpg\" % i, image)\n",
    "                plt.imshow(image)'''\n",
    "                predicted_box = cv2.rectangle(image, (int(new_mask[0]), int(new_mask[1])),\n",
    "                              (int(new_mask[2]), int(new_mask[3])), (0, 0, 255), 2)\n",
    "                #predicted_result = cv2.bitwise_and(image, image, predicted_box)\n",
    "\n",
    "                groundtruth= cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                              (int(bounding_box[2]), int(bounding_box[3])), (0, 255, 0), 2)\n",
    "                test_result = cv2.bitwise_and(image, image, groundtruth)\n",
    "                \n",
    "                plt.imshow(test_result)\n",
    "                plt.title('predicted box is shown in bold blue and ground truth box is shown in bold green \\n Search path shown in red')\n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "            cropped_image = crop_image(image, new_mask)\n",
    "            feature = extract_feature(cropped_image, history, vgg16)\n",
    "\n",
    "            masks.append(new_mask)\n",
    "            current_mask = new_mask\n",
    "            cv2.rectangle(image, (int(current_mask[0]), int(current_mask[1])),\n",
    "                          (int(current_mask[2]), int(current_mask[3])), (255, 0, 0), 1)\n",
    "            step += 1\n",
    "\n",
    "        mask = masks[-1]\n",
    "        iou.append(compute_iou(mask,bounding_box))\n",
    "\n",
    "\n",
    "    print(sum(iou)/len(iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3AmAofnCT6Q"
   },
   "source": [
    "**Testing the agent on 100 test images and calculating the average iou:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ea684T21wrhi"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def test():\n",
    "    object_number = 1\n",
    "    image_list, bounding_box_list = load_data(object_number,test=True)\n",
    "    iou = []\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(\"model.h5\")\n",
    "\n",
    "    for i in range(0, 100):\n",
    "        bounding_box = bounding_box_list[i]\n",
    "        image = image_list[i]\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        current_mask = np.asarray([0, 0, width, height])\n",
    "        feature = extract_feature(image, history, vgg16)\n",
    "        end = False\n",
    "        masks = []\n",
    "        step = 0\n",
    "\n",
    "        while not end:\n",
    "\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "\n",
    "            action = np.argmax(q_value)\n",
    "\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            if action == 8 or step == 10:\n",
    "                end = True\n",
    "                print(\"end\")\n",
    "                new_mask = current_mask\n",
    "                cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                              (int(bounding_box[2]), int(bounding_box[3])), (0, 0, 255), 1)\n",
    "                cv2.imwrite(\"./result/plane_result%d.jpg\" % i, image)\n",
    "            else:\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "            cropped_image = crop_image(image, new_mask)\n",
    "            feature = extract_feature(cropped_image, history, vgg16)\n",
    "\n",
    "            masks.append(new_mask)\n",
    "            current_mask = new_mask\n",
    "            cv2.rectangle(image, (int(current_mask[0]), int(current_mask[1])),\n",
    "                          (int(current_mask[2]), int(current_mask[3])), (0, 255, 0), 1)\n",
    "            step += 1\n",
    "\n",
    "        mask = masks[-1]\n",
    "        iou.append(compute_iou(mask,bounding_box))\n",
    "\n",
    "\n",
    "    print(sum(iou)/len(iou))\n",
    "    cv2.rectangle(image, (int(mask[0]), int(mask[1])),\n",
    "                   (int(mask[2]),int(mask[3])),(0, 255, 0), 2)\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RI0HVRfCbl3"
   },
   "source": [
    "**Training the agent:** \n",
    "(to see the images of test results scroll down in the output box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hhuH-0xbwqDy",
    "outputId": "2a270d21-264b-445a-e07e-a83b7cdee6fa"
   },
   "outputs": [],
   "source": [
    "history_size = 10\n",
    "action_option = 9\n",
    "max_steps = 20\n",
    "experience_sample_size = 20\n",
    "max_experience_size = 1000\n",
    "gamma = 0.1\n",
    "epsilon_change_steps = 10\n",
    "loss_arr = []\n",
    "\n",
    "\n",
    "def extract_feature(image, history, vgg16):\n",
    "    history_feature = np.zeros(action_option * history_size)\n",
    "    for i in range(history_size):\n",
    "        if history[i] != -1:\n",
    "            history_feature[i * action_option + history[i]] = 1\n",
    "\n",
    "    feature_extractor = K.function([vgg16.layers[0].input], [vgg16.layers[20].output])\n",
    "    image_reshape = [(cv2.resize(image, (224, 224))).reshape(1, 224, 224, 3)]\n",
    "    image_feature = feature_extractor(image_reshape)[0]\n",
    "    image_feature = np.ndarray.flatten(image_feature)\n",
    "    feature = np.concatenate((image_feature, history_feature))\n",
    "\n",
    "    return np.array([feature])\n",
    "\n",
    "\n",
    "def compute_q(feature, deep_q_model):\n",
    "    output = deep_q_model.predict(feature)\n",
    "    return np.ndarray.flatten(output)\n",
    "\n",
    "\n",
    "def compute_mask(action, current_mask):\n",
    "    image_rate = 0.1\n",
    "    delta_width = image_rate * (current_mask[2] - current_mask[0])\n",
    "    delta_height = image_rate * (current_mask[3] - current_mask[1])\n",
    "    dx1 = 0\n",
    "    dy1 = 0\n",
    "    dx2 = 0\n",
    "    dy2 = 0\n",
    "\n",
    "    if action == 0:\n",
    "        dx1 = delta_width\n",
    "        dx2 = delta_width\n",
    "    elif action == 1:\n",
    "        dx1 = -delta_width\n",
    "        dx2 = -delta_width\n",
    "    elif action == 2:\n",
    "        dy1 = delta_height\n",
    "        dy2 = delta_height\n",
    "    elif action == 3:\n",
    "        dy1 = -delta_height\n",
    "        dy2 = -delta_height\n",
    "    elif action == 4:\n",
    "        dx1 = -delta_width\n",
    "        dx2 = delta_width\n",
    "        dy1 = -delta_height\n",
    "        dy2 = delta_height\n",
    "    elif action == 5:\n",
    "        dx1 = delta_width\n",
    "        dx2 = -delta_width\n",
    "        dy1 = delta_height\n",
    "        dy2 = -delta_height\n",
    "    elif action == 6:\n",
    "        dy1 = delta_height\n",
    "        dy2 = -delta_height\n",
    "    elif action == 7:\n",
    "        dx1 = delta_width\n",
    "        dx2 = -delta_width\n",
    "\n",
    "    new_mask_tmp = np.array([current_mask[0] + dx1, current_mask[1] + dy1,\n",
    "                         current_mask[2] + dx2, current_mask[3] + dy2])\n",
    "    new_mask = np.array([\n",
    "        min(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        min(new_mask_tmp[1], new_mask_tmp[3]),\n",
    "        max(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        max(new_mask_tmp[1], new_mask_tmp[3])\n",
    "    ])\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "def compute_iou(mask, ground_truth):\n",
    "    dx = min(mask[2], ground_truth[2]) - max(mask[0], ground_truth[0])\n",
    "    dy = min(mask[3], ground_truth[3]) - max(mask[1], ground_truth[1])\n",
    "\n",
    "    if (dx >= 0) and (dy >= 0):\n",
    "        inter_area = dx*dy\n",
    "    else:\n",
    "        inter_area = 0\n",
    "\n",
    "    mask_area = (mask[2] - mask[0]) * (mask[3] - mask[1])\n",
    "    ground_truth_area = (ground_truth[2] - ground_truth[0]) * (ground_truth[3] - ground_truth[1])\n",
    "\n",
    "    return inter_area / (mask_area + ground_truth_area - inter_area)\n",
    "\n",
    "\n",
    "def compute_reward(action, ground_truth, current_mask):\n",
    "    new_mask = compute_mask(action, current_mask)\n",
    "    iou_new = compute_iou(new_mask, ground_truth)\n",
    "    iou_current = compute_iou(current_mask, ground_truth)\n",
    "\n",
    "    if iou_current < iou_new:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def compute_end_reward(current_mask, ground_truth):\n",
    "    if compute_iou(current_mask, ground_truth) > 0.5:\n",
    "        return 3\n",
    "    else:\n",
    "        return -3\n",
    "\n",
    "\n",
    "def select_action(feature, ground_truth_box, step, q_value, epsilon, current_mask):\n",
    "    if step == max_steps:\n",
    "        action = 8 #select trigger if agent surpassed maximum number of steps\n",
    "\n",
    "    else:\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(q_value)\n",
    "        else:\n",
    "            end_reward = compute_end_reward(current_mask, ground_truth_box)\n",
    "            if end_reward > 0:\n",
    "                action = 8\n",
    "            else:\n",
    "                rewards = []\n",
    "                for i in range(action_option - 1):\n",
    "                    reward = compute_reward(i, ground_truth_box, current_mask)\n",
    "                    rewards.append(reward)\n",
    "                rewards = np.asarray(rewards)\n",
    "                positive_reward_index = np.where(rewards >= 0)[0]\n",
    "\n",
    "                if len(positive_reward_index) == 0:\n",
    "                    positive_reward_index = np.asarray(range(9))\n",
    "\n",
    "                action = np.random.choice(positive_reward_index)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def execute_action(action, history, ground_truth_box, current_mask):\n",
    "    if action == 8:\n",
    "        new_mask = current_mask\n",
    "        reward = compute_end_reward(current_mask, ground_truth_box)\n",
    "        end = True\n",
    "    else:\n",
    "        new_mask = compute_mask(action, current_mask)\n",
    "        reward = compute_reward(action, ground_truth_box, current_mask)\n",
    "        history = history[1:]\n",
    "        history.append(action)\n",
    "        end = False\n",
    "\n",
    "    return new_mask, reward, end, history\n",
    "\n",
    "\n",
    "def compute_target(reward, new_feature, model):\n",
    "    return reward + gamma * np.amax(compute_q(new_feature, model))\n",
    "\n",
    "\n",
    "def crop_image(image, new_mask):\n",
    "    height, width, channel = np.shape(image)\n",
    "    new_mask = np.asarray(new_mask).astype(\"int\")\n",
    "    new_mask[0] = max(new_mask[0], 0)\n",
    "    new_mask[1] = max(new_mask[1], 0)\n",
    "    new_mask[2] = min(new_mask[2], width)\n",
    "    new_mask[3] = min(new_mask[3], height)\n",
    "    cropped_image = image[new_mask[1]:new_mask[3], new_mask[0]:new_mask[2]]\n",
    "    new_height, new_width, new_channel = np.shape(cropped_image)\n",
    "\n",
    "    if new_height == 0 or new_width == 0:\n",
    "        cropped_image = np.zeros((224, 224, 3))\n",
    "    else:\n",
    "        cv2.resize(cropped_image, (224, 224))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def experience_replay(deep_q_model, experience):\n",
    "    sample = random.choices(experience, k=experience_sample_size)\n",
    "\n",
    "    targets = np.zeros((experience_sample_size, action_option))\n",
    "\n",
    "    for i in range(experience_sample_size):\n",
    "        feature, action, new_feature, reward, end = sample[i]\n",
    "        target = reward\n",
    "\n",
    "        if not end:\n",
    "            target = compute_target(reward, new_feature, deep_q_model)\n",
    "\n",
    "        targets[i, :] = compute_q(feature, deep_q_model)\n",
    "        targets[i][action] = target\n",
    "\n",
    "    x = np.concatenate([each[0] for each in sample])\n",
    "\n",
    "    global loss_arr\n",
    "    loss = deep_q_model.train_on_batch(x, targets)\n",
    "    loss_arr.append(loss)\n",
    "    if len(loss_arr) == 100:\n",
    "        print(\"loss %s\" % str(sum(loss_arr) / len(loss_arr)))\n",
    "        loss_arr = []\n",
    "\n",
    "\n",
    "def train_deep_q(training_epoch, epsilon, image_list, bounding_box_list, deep_q_model, vgg16):\n",
    "    experience = []\n",
    "\n",
    "    for current_epoch in range(1, training_epoch + 1):\n",
    "\n",
    "        print(\"Now starting epoch %d\" % current_epoch)\n",
    "        training_set_size = np.shape(image_list)[0]\n",
    "\n",
    "        for i in range(1000):\n",
    "            image = image_list[i]\n",
    "            ground_truth_box = bounding_box_list[i]\n",
    "            history = [-1] * history_size\n",
    "            height, width, channel = np.shape(image)\n",
    "            current_mask = np.asarray([0, 0, width, height])\n",
    "            feature = extract_feature(image, history, vgg16)\n",
    "            end = False\n",
    "            step = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            while not end:\n",
    "                q_value = compute_q(feature, deep_q_model)\n",
    "                action = select_action(feature, ground_truth_box, step, q_value, epsilon, current_mask)\n",
    "                new_mask, reward, end, history = execute_action(action, history, ground_truth_box, current_mask)\n",
    "                cropped_image = crop_image(image, new_mask)\n",
    "                new_feature = extract_feature(cropped_image, history, vgg16)\n",
    "                if len(experience) > max_experience_size:\n",
    "                    experience = experience[1:]\n",
    "                    experience.append([feature, action, new_feature, reward, end])\n",
    "                else:\n",
    "                    experience.append([feature, action, new_feature, reward, end])\n",
    "\n",
    "                experience_replay(deep_q_model, experience)\n",
    "                feature = new_feature\n",
    "                current_mask = new_mask\n",
    "                step += 1\n",
    "                total_reward += reward\n",
    "\n",
    "            print(\"Image %d, total reward %i\" % (i, total_reward))\n",
    "\n",
    "        if current_epoch < epsilon_change_steps:\n",
    "            epsilon -= 0.1\n",
    "            print(\"current epsilon is %f\" % epsilon)\n",
    "\n",
    "        tf.keras.models.save_model(deep_q_model, \"my_tmp_model.h5\")\n",
    "\n",
    "    return deep_q_model\n",
    "\n",
    "\n",
    "HUBER_DELTA = 1.0\n",
    "def smoothL1(y_true, y_pred):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    x = tf.where(x < HUBER_DELTA, 0.5 * x ** 2, HUBER_DELTA * (x - 0.5 * HUBER_DELTA))\n",
    "    return K.sum(x)\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(4096 + action_option*history_size,), activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(9, activation='linear'))\n",
    "    model.compile(loss=smoothL1, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vgg16():\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=True , pooling='max')\n",
    "    #vgg16.summary()\n",
    "    return vgg16\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # object_number = int(sys.argv[1])\n",
    "    # training_epoch = sys.argv[2]\n",
    "    # epsilon = sys.argv[3]\n",
    "    training_epoch = 10\n",
    "    epsilon = 1\n",
    "    image_list = []\n",
    "    bounding_box_list = []\n",
    "    for i in range(1,21):\n",
    "        image_data, bounding_box_data = load_data(i, test=False)\n",
    "        image_list.append(image_data)\n",
    "        bounding_box_list.append(bounding_box_data)\n",
    "    deep_q_model = create_q_model()\n",
    "    vgg16 = create_vgg16()\n",
    "    trained_model = train_deep_q(training_epoch, epsilon, image_list, bounding_box_list, deep_q_model, vgg16)\n",
    "    trained_model.save(\"model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 10\n",
    "action_option = 9\n",
    "max_steps = 20\n",
    "experience_sample_size = 20\n",
    "max_experience_size = 1000\n",
    "gamma = 0.1\n",
    "epsilon_change_steps = 10\n",
    "loss_arr = []\n",
    "def test2(iou_threshold):\n",
    "    image_list, bounding_box_list = load_data(test=True)\n",
    "    iou = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(\"model.h5\")\n",
    "    all_predicted_boxes = []\n",
    "    all_groundtruth_boxes = []\n",
    "\n",
    "    for i in range(101):\n",
    "        bounding_box = bounding_box_list[i]\n",
    "        image = image_list[i]\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        current_mask = np.asarray([0, 0, width, height])\n",
    "        feature = extract_feature(image, history, vgg16)\n",
    "        end = False\n",
    "        masks = []\n",
    "        predicted_boxes = []\n",
    "\n",
    "        for step in range(40):  # fixed number of steps\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "            action = np.argmax(q_value)\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            if action == 8 or step == 39:  # fixed number of steps\n",
    "                end = True\n",
    "                predicted_box = current_mask.tolist()\n",
    "                predicted_boxes.append(predicted_box)\n",
    "                all_predicted_boxes.append(predicted_box)\n",
    "                break\n",
    "            else:\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "                cropped_image = crop_image(image, new_mask)\n",
    "                feature = extract_feature(cropped_image, history, vgg16)\n",
    "                masks.append(new_mask)\n",
    "                predicted_box = new_mask.tolist()\n",
    "                predicted_boxes.append(predicted_box)\n",
    "                current_mask = new_mask\n",
    "\n",
    "        if not end:\n",
    "            predicted_box = current_mask.tolist()\n",
    "            predicted_boxes.append(predicted_box)\n",
    "            all_predicted_boxes.append(predicted_box)\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for predicted_box in predicted_boxes:\n",
    "            iou_scores = [compute_iou(predicted_box, gt_box) for gt_box in bounding_box_list]\n",
    "            max_iou = max(iou_scores)\n",
    "            if max_iou >= iou_threshold:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        fn = len(bounding_box_list) - tp\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        iou.append(max_iou)\n",
    "\n",
    "    # Calculate AP for each class\n",
    "    ap_list = []\n",
    "    iou_thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "    for iou_thresh in iou_thresholds:\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        total_groundtruth_boxes = len(bounding_box_list)\n",
    "\n",
    "        for i in range(len(all_predicted_boxes)):\n",
    "            iou_score = compute_iou(all_predicted_boxes[i], bounding_box_list[i])\n",
    "            if iou_score >= iou_thresh:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "        if total_groundtruth_boxes == 0:\n",
    "            average_precision = 0\n",
    "        else:\n",
    "            average_precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "        ap_list.append(average_precision)\n",
    "\n",
    "    mAP = np.mean(ap_list)\n",
    "    return mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP = test2(0.5, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
