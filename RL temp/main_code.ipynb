{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryzoRoA-iuoB"
   },
   "source": [
    "# **Active Object Localization with Deep Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import tensorflow.compat.v1 as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*7))])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\") \n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "yeqbT99oVBC7",
    "outputId": "a196e124-8e60-4682-db7c-21139be74f6e"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download required dataset \n",
    "!kaggle datasets download -d huanghanchina/pascal-voc-2012 -p pascal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xB7qEvokBqgx"
   },
   "source": [
    "**Extracting the zip file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgqLDYH2xcda"
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "os.chdir('./pascal')  #change dir\n",
    "!unzip -q pascal-voc-2012.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./pascal')  #change dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhS-AMuLByO1"
   },
   "source": [
    "**Install and import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "PR8qwxz6yOjs",
    "outputId": "876c76ca-7c97-411a-9c3d-0172f588011d"
   },
   "outputs": [],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHJNJjhYiPK8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import xmltodict\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTaePMB5B3zu"
   },
   "source": [
    "**Prepare and loading images from dataset along with their bounding boxes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjG6qcg1wM75"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_object_name(object_number):\n",
    "    \"\"\"\n",
    "    Converting object's id to object's name\n",
    "    \"\"\"\n",
    "    # Define a dictionary that maps object numbers to object names\n",
    "    dictionary={ 1:'aeroplane' , 2:'bicycle', 3:'bird', 4:'boat', \n",
    "                 5:'bottle' , 6:'bus', 7:'car', 8:'cat', 9:'chair', \n",
    "                 10:'cow', 11:'diningtable', 12:'dog' , 13:'horse', \n",
    "                 14:'motorbike', 15:'person', 16:'pottedplant',\n",
    "                 17:'sheep', 18:'sofa', 19:'train', 20:'tvmonitor'}\n",
    "\n",
    "    # Return the object name corresponding to the input object number\n",
    "    return dictionary[object_number]\n",
    "\n",
    "def read_image_index(object_name, dataset_path, test):\n",
    "    \"\"\"\n",
    "    Reading the name of images from the txt file of target object\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the names of images containing the target object\n",
    "    index_list = []\n",
    "\n",
    "    # Define the path to the text file that contains the image names\n",
    "    if test:\n",
    "        # If test is True, use the trainval.txt file\n",
    "        index_file_path = dataset_path + \"ImageSets/Main/\" + object_name + \"_trainval.txt\"\n",
    "    else:\n",
    "        # If test is False, use the train.txt file\n",
    "        index_file_path = dataset_path + \"ImageSets/Main/\" + object_name + \"_train.txt\"\n",
    "\n",
    "    # Open the text file\n",
    "    with open(index_file_path, 'r') as f:\n",
    "        # Loop through each line of the text file\n",
    "        for line in f:\n",
    "            # Check if the line contains the target object\n",
    "            if \"-1\" not in line.split(\" \")[1]:\n",
    "                # If the line contains the target object, extract the image name and add it to the index list\n",
    "                index_list.append(line.split(\" \")[0])\n",
    "\n",
    "    # Return the list of image names\n",
    "    return index_list\n",
    "\n",
    "def read_image(image_index, dataset_path):\n",
    "    \"\"\"\n",
    "    Loading images using their name from JPEGImages folder\n",
    "\n",
    "    :param image_index: list of image names\n",
    "    :param dataset_path: path to dataset\n",
    "    :return: list of images\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    image_folder_path = dataset_path + \"JPEGImages/\"\n",
    "    for each_image in image_index:\n",
    "        img = cv2.imread(image_folder_path + each_image + \".jpg\")\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "def load_annotation(image_index, object_name, dataset_path):\n",
    "    \"\"\"\n",
    "    Loading bounding boxes around objects in images\n",
    "\n",
    "    :param image_index: list of image names\n",
    "    :param object_name: name of the object to be detected\n",
    "    :param dataset_path: path to dataset\n",
    "    :return: list of bounding boxes\n",
    "    \"\"\"\n",
    "    bounding_box_list = []\n",
    "    annotattion_path = dataset_path + \"Annotations/\"\n",
    "    for each_image in image_index:\n",
    "        path = annotattion_path + each_image + \".xml\"\n",
    "        xml = xmltodict.parse(open(path, 'rb'))\n",
    "        xml_objects = xml['annotation']['object']\n",
    "        if isinstance(xml_objects, list):\n",
    "            for each_object in xml_objects:\n",
    "                if each_object[\"name\"] == object_name:\n",
    "                    xmin = each_object[\"bndbox\"][\"xmin\"]\n",
    "                    ymin = each_object[\"bndbox\"][\"ymin\"]\n",
    "                    xmax = each_object[\"bndbox\"][\"xmax\"]\n",
    "                    ymax = each_object[\"bndbox\"][\"ymax\"]\n",
    "                    bounding_box = (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "                    bounding_box_list.append(bounding_box)\n",
    "                    break\n",
    "        else:\n",
    "            if xml_objects[\"name\"] == object_name:\n",
    "                xmin = xml_objects[\"bndbox\"][\"xmin\"]\n",
    "                ymin = xml_objects[\"bndbox\"][\"ymin\"]\n",
    "                xmax = xml_objects[\"bndbox\"][\"xmax\"]\n",
    "                ymax = xml_objects[\"bndbox\"][\"ymax\"]\n",
    "                bounding_box = (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "                bounding_box_list.append(bounding_box)\n",
    "\n",
    "    return bounding_box_list\n",
    "\n",
    "def load_data(object_number, test):\n",
    "\n",
    "    \"\"\"\n",
    "    Loading dataset images for a specific class by calling corresponding functions\n",
    "    and saving images and their annotations into arrays\n",
    "    \"\"\" \n",
    "    dataset_path = \"./VOC2012/\"\n",
    "    object_name = get_object_name(object_number)  \n",
    "    image_index = read_image_index(object_name, dataset_path,test)\n",
    "    #print(len(image_index))\n",
    "    image_list = np.asarray(read_image(image_index, dataset_path))\n",
    "    bounding_box_list = np.asarray(load_annotation(image_index, object_name, dataset_path))\n",
    "    \n",
    "    if test:\n",
    "      np.save(object_name + \"_valimage.npy\", image_list)\n",
    "      np.save(object_name + \"_valbox.npy\", bounding_box_list)\n",
    "\n",
    "    else:\n",
    "      np.save(object_name + \"_image.npy\", image_list)\n",
    "      np.save(object_name + \"_box.npy\", bounding_box_list)\n",
    "\n",
    "    return image_list, bounding_box_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STknfrr7CBvf"
   },
   "source": [
    "**Showing some pictures from 20 classes of dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "colab_type": "code",
    "id": "amoVpSmoDJFs",
    "outputId": "85764130-76ba-41a3-d23e-9efe7d3543cf"
   },
   "outputs": [],
   "source": [
    "# Set the size of the figure to be displayed\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "# Loop through 20 sets of image data\n",
    "for i in range(1,21):\n",
    "    \n",
    "    # Load the i-th set of image data and bounding box information\n",
    "    image_list, bounding_box_list = load_data(i,test=False)\n",
    "    \n",
    "    # Select the 15th image in the set\n",
    "    im = image_list[15]\n",
    "    \n",
    "    # Draw a rectangle with blue line borders of thickness of 3 px around the object of interest\n",
    "    mask = cv2.rectangle(im, (bounding_box_list[15][0], bounding_box_list[15][1]), (bounding_box_list[15][2], bounding_box_list[15][3])  , (255, 0, 0) , 3) \n",
    "    \n",
    "    # Apply the mask to the image data\n",
    "    masked_data = cv2.bitwise_and(im, im, mask)\n",
    "    \n",
    "    # Display the masked image\n",
    "    plt.subplot(5,4,i)\n",
    "    plt.imshow(masked_data)\n",
    "    \n",
    "    # Get the object name for the i-th set of image data\n",
    "    label = get_object_name(i)\n",
    "    \n",
    "    # Set the title of the plot to be the object name\n",
    "    plt.title(str(label))\n",
    "\n",
    "# Show the complete figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsjIZeidCI1m"
   },
   "source": [
    "**Showing 10 sample test images (this funciton is called after training the agent and testing it on 100 test images)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7xrLZacUbGv"
   },
   "outputs": [],
   "source": [
    "def test2():\n",
    "    # Load the test data for object 1\n",
    "    object_number = 1\n",
    "    image_list, bounding_box_list = load_data(object_number ,test=True)\n",
    "    iou = []\n",
    "\n",
    "    # Load the VGG16 and Deep Q models\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(\"model.h5\")\n",
    "\n",
    "    # Loop through a subset of images in the test data\n",
    "    for i in range(30,40):\n",
    "        bounding_box = bounding_box_list[i]\n",
    "        image = image_list[i]\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        current_mask = np.asarray([0, 0, width, height])\n",
    "        feature = extract_feature(image, history, vgg16)\n",
    "        end = False\n",
    "        masks = []\n",
    "        step = 0\n",
    "        \n",
    "        # Keep taking actions until the end is reached\n",
    "        while not end:\n",
    "\n",
    "            # Compute the Q values for the current state\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "\n",
    "            # Select the action with the highest Q value\n",
    "            action = np.argmax(q_value)\n",
    "\n",
    "            # Update the history with the selected action\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            # If the end state is reached or the maximum number of steps is reached\n",
    "            if action == 8 or step == 10: #steps should be changed to 40\n",
    "                end = True\n",
    "\n",
    "                # Visualize the predicted box and ground truth box for the current image\n",
    "                plt.figure()\n",
    "                new_mask = current_mask\n",
    "                cv2.rectangle(image, (int(new_mask[0]), int(new_mask[1])),\n",
    "                              (int(new_mask[2]), int(new_mask[3])), (255, 0, 0), 1)\n",
    "\n",
    "                predicted_box = cv2.rectangle(image, (int(new_mask[0]), int(new_mask[1])),\n",
    "                              (int(new_mask[2]), int(new_mask[3])), (0, 0, 255), 2)\n",
    "\n",
    "                groundtruth= cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                              (int(bounding_box[2]), int(bounding_box[3])), (0, 255, 0), 2)\n",
    "\n",
    "                test_result = cv2.bitwise_and(image, image, groundtruth)\n",
    "                \n",
    "                plt.imshow(test_result)\n",
    "                plt.title('predicted box is shown in bold blue and ground truth box is shown in bold green \\n Search path shown in red')\n",
    "                plt.show()\n",
    "\n",
    "            # If the end state is not reached\n",
    "            else:\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "            # Crop the image based on the new mask\n",
    "            cropped_image = crop_image(image, new_mask)\n",
    "\n",
    "            # Extract features from the cropped image and update the current mask\n",
    "            feature = extract_feature(cropped_image, history, vgg16)\n",
    "            masks.append(new_mask)\n",
    "            current_mask = new_mask\n",
    "\n",
    "            # Visualize the search path\n",
    "            cv2.rectangle(image, (int(current_mask[0]), int(current_mask[1])),\n",
    "                          (int(current_mask[2]), int(current_mask[3])), (255, 0, 0), 1)\n",
    "            step += 1\n",
    "\n",
    "        # Compute the IoU for the predicted box and the ground truth box and store it in the iou list\n",
    "        mask = masks[-1]\n",
    "        iou.append(compute_iou(mask,bounding_box))\n",
    "\n",
    "    # Compute the average IoU for all images in the test set\n",
    "    print(sum(iou)/len(iou))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3AmAofnCT6Q"
   },
   "source": [
    "**Testing the agent on 100 test images and calculating the average iou:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ea684T21wrhi"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the test function\n",
    "def test():\n",
    "    # Set the object number and load the test data\n",
    "    object_number = 1\n",
    "    image_list, bounding_box_list = load_data(object_number,test=True)\n",
    "\n",
    "    # Initialize the intersection over union (IOU) list and create VGG16 and Q models\n",
    "    iou = []\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "\n",
    "    # Load the pretrained weights for the Q model\n",
    "    deep_q.load_weights(\"model.h5\")\n",
    "\n",
    "    # Iterate through each image in the test set\n",
    "    for i in range(0, 100):\n",
    "        # Get the bounding box and image\n",
    "        bounding_box = bounding_box_list[i]\n",
    "        image = image_list[i]\n",
    "\n",
    "        # Initialize the history list, current mask, and step count\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        current_mask = np.asarray([0, 0, width, height])\n",
    "        step = 0\n",
    "\n",
    "        # Set the end flag to False and initialize the masks list\n",
    "        end = False\n",
    "        masks = []\n",
    "\n",
    "        # While not at the end, keep predicting new masks\n",
    "        while not end:\n",
    "            # Compute the Q value for the current state\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "\n",
    "            # Choose the action with the highest Q value\n",
    "            action = np.argmax(q_value)\n",
    "\n",
    "            # Update the history list and append the chosen action\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            # If the action is 8 (stop) or the step count reaches 10, end the loop\n",
    "            if action == 8 or step == 10:\n",
    "                end = True\n",
    "                print(\"end\")\n",
    "                # Save the image with the predicted bounding box\n",
    "                new_mask = current_mask\n",
    "                cv2.rectangle(image, (int(bounding_box[0]), int(bounding_box[1])),\n",
    "                              (int(bounding_box[2]), int(bounding_box[3])), (0, 0, 255), 1)\n",
    "                cv2.imwrite(\"./result/plane_result%d.jpg\" % i, image)\n",
    "            else:\n",
    "                # Compute the new mask based on the chosen action\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "            # Crop the image based on the new mask and extract features from the cropped image\n",
    "            cropped_image = crop_image(image, new_mask)\n",
    "            feature = extract_feature(cropped_image, history, vgg16)\n",
    "\n",
    "            # Append the new mask to the masks list, update the current mask, and increment the step count\n",
    "            masks.append(new_mask)\n",
    "            current_mask = new_mask\n",
    "            cv2.rectangle(image, (int(current_mask[0]), int(current_mask[1])),\n",
    "                          (int(current_mask[2]), int(current_mask[3])), (0, 255, 0), 1)\n",
    "            step += 1\n",
    "\n",
    "        # Compute the IOU between the predicted mask and the ground truth bounding box and append to the IOU list\n",
    "        mask = masks[-1]\n",
    "        iou.append(compute_iou(mask,bounding_box))\n",
    "\n",
    "    # Print the average IOU score\n",
    "    print(sum(iou)/len(iou))\n",
    "\n",
    "    # Draw the final bounding box on the image and display it\n",
    "    cv2.rectangle(image, (int(mask[0]), int(mask[1])),\n",
    "                   (int(mask[2]),int(mask[3])),(0, 255, 0), 2)\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RI0HVRfCbl3"
   },
   "source": [
    "**Training the agent:** \n",
    "(to see the images of test results scroll down in the output box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hhuH-0xbwqDy",
    "outputId": "2a270d21-264b-445a-e07e-a83b7cdee6fa"
   },
   "outputs": [],
   "source": [
    "# Set parameters for the Deep Q-Learning algorithm\n",
    "history_size = 10                 # number of previous actions to consider\n",
    "action_option = 9                # number of possible actions agent can take\n",
    "max_steps = 20                   # maximum number of steps in one episode\n",
    "experience_sample_size = 20       # size of experience sample to train on\n",
    "max_experience_size = 1000       # maximum size of experience replay buffer\n",
    "gamma = 0.1                      # discount factor\n",
    "epsilon_change_steps = 10         # number of steps before decreasing epsilon\n",
    "loss_arr = []                     # list to store loss values during training\n",
    "\n",
    "\n",
    "# Define function to extract features from image and previous actions\n",
    "def extract_feature(image, history, vgg16):\n",
    "    # Initialize feature vector for history\n",
    "    history_feature = np.zeros(action_option * history_size)\n",
    "    # Loop through history to encode previous actions\n",
    "    for i in range(history_size):\n",
    "        if history[i] != -1:\n",
    "            history_feature[i * action_option + history[i]] = 1\n",
    "    \n",
    "    # Extract image features using pre-trained VGG16 model\n",
    "    feature_extractor = K.function([vgg16.layers[0].input], [vgg16.layers[20].output])\n",
    "    image_reshape = [(cv2.resize(image, (224, 224))).reshape(1, 224, 224, 3)]\n",
    "    image_feature = feature_extractor(image_reshape)[0]\n",
    "    # Flatten and concatenate image and history features\n",
    "    image_feature = np.ndarray.flatten(image_feature)\n",
    "    feature = np.concatenate((image_feature, history_feature))\n",
    "    \n",
    "    return np.array([feature])\n",
    "\n",
    "\n",
    "# Define function to compute Q-values for given state using deep Q-network model\n",
    "def compute_q(feature, deep_q_model):\n",
    "    output = deep_q_model.predict(feature)\n",
    "    return np.ndarray.flatten(output)\n",
    "\n",
    "\n",
    "\n",
    "def compute_mask(action, current_mask):\n",
    "    # Define image rate, which is a hyperparameter to adjust the amount of mask movement for each action\n",
    "    image_rate = 0.1\n",
    "    \n",
    "    # Calculate the width and height differences based on the current mask\n",
    "    delta_width = image_rate * (current_mask[2] - current_mask[0])\n",
    "    delta_height = image_rate * (current_mask[3] - current_mask[1])\n",
    "    \n",
    "    # Initialize the delta values for x and y coordinates\n",
    "    dx1 = 0\n",
    "    dy1 = 0\n",
    "    dx2 = 0\n",
    "    dy2 = 0\n",
    "\n",
    "    # Determine the delta values based on the action\n",
    "    if action == 0:\n",
    "        dx1 = delta_width\n",
    "        dx2 = delta_width\n",
    "    elif action == 1:\n",
    "        dx1 = -delta_width\n",
    "        dx2 = -delta_width\n",
    "    elif action == 2:\n",
    "        dy1 = delta_height\n",
    "        dy2 = delta_height\n",
    "    elif action == 3:\n",
    "        dy1 = -delta_height\n",
    "        dy2 = -delta_height\n",
    "    elif action == 4:\n",
    "        dx1 = -delta_width\n",
    "        dx2 = delta_width\n",
    "        dy1 = -delta_height\n",
    "        dy2 = delta_height\n",
    "    elif action == 5:\n",
    "        dx1 = delta_width\n",
    "        dx2 = -delta_width\n",
    "        dy1 = delta_height\n",
    "        dy2 = -delta_height\n",
    "    elif action == 6:\n",
    "        dy1 = delta_height\n",
    "        dy2 = -delta_height\n",
    "    elif action == 7:\n",
    "        dx1 = delta_width\n",
    "        dx2 = -delta_width\n",
    "\n",
    "    # Calculate the new mask based on the delta values and ensure that it's in the correct format\n",
    "    new_mask_tmp = np.array([current_mask[0] + dx1, current_mask[1] + dy1,\n",
    "                         current_mask[2] + dx2, current_mask[3] + dy2])\n",
    "    new_mask = np.array([\n",
    "        min(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        min(new_mask_tmp[1], new_mask_tmp[3]),\n",
    "        max(new_mask_tmp[0], new_mask_tmp[2]),\n",
    "        max(new_mask_tmp[1], new_mask_tmp[3])\n",
    "    ])\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "def compute_iou(mask, ground_truth):\n",
    "    dx = min(mask[2], ground_truth[2]) - max(mask[0], ground_truth[0])\n",
    "    dy = min(mask[3], ground_truth[3]) - max(mask[1], ground_truth[1])\n",
    "\n",
    "    if (dx >= 0) and (dy >= 0):\n",
    "        inter_area = dx*dy\n",
    "    else:\n",
    "        inter_area = 0\n",
    "\n",
    "    mask_area = (mask[2] - mask[0]) * (mask[3] - mask[1])\n",
    "    ground_truth_area = (ground_truth[2] - ground_truth[0]) * (ground_truth[3] - ground_truth[1])\n",
    "\n",
    "    return inter_area / (mask_area + ground_truth_area - inter_area)\n",
    "\n",
    "\n",
    "def compute_reward(action, ground_truth, current_mask):\n",
    "    new_mask = compute_mask(action, current_mask)\n",
    "    iou_new = compute_iou(new_mask, ground_truth)\n",
    "    iou_current = compute_iou(current_mask, ground_truth)\n",
    "\n",
    "    if iou_current < iou_new:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def compute_end_reward(current_mask, ground_truth):\n",
    "    if compute_iou(current_mask, ground_truth) > 0.5:\n",
    "        return 3\n",
    "    else:\n",
    "        return -3\n",
    "\n",
    "\n",
    "def select_action(feature, ground_truth_box, step, q_value, epsilon, current_mask):\n",
    "    if step == max_steps:\n",
    "        action = 8 #select trigger if agent surpassed maximum number of steps\n",
    "\n",
    "    else:\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(q_value)\n",
    "        else:\n",
    "            end_reward = compute_end_reward(current_mask, ground_truth_box)\n",
    "            if end_reward > 0:\n",
    "                action = 8\n",
    "            else:\n",
    "                rewards = []\n",
    "                for i in range(action_option - 1):\n",
    "                    reward = compute_reward(i, ground_truth_box, current_mask)\n",
    "                    rewards.append(reward)\n",
    "                rewards = np.asarray(rewards)\n",
    "                positive_reward_index = np.where(rewards >= 0)[0]\n",
    "\n",
    "                if len(positive_reward_index) == 0:\n",
    "                    positive_reward_index = np.asarray(range(9))\n",
    "\n",
    "                action = np.random.choice(positive_reward_index)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def execute_action(action, history, ground_truth_box, current_mask):\n",
    "    if action == 8:\n",
    "        new_mask = current_mask\n",
    "        reward = compute_end_reward(current_mask, ground_truth_box)\n",
    "        end = True\n",
    "    else:\n",
    "        new_mask = compute_mask(action, current_mask)\n",
    "        reward = compute_reward(action, ground_truth_box, current_mask)\n",
    "        history = history[1:]\n",
    "        history.append(action)\n",
    "        end = False\n",
    "\n",
    "    return new_mask, reward, end, history\n",
    "\n",
    "\n",
    "def compute_target(reward, new_feature, model):\n",
    "    return reward + gamma * np.amax(compute_q(new_feature, model))\n",
    "\n",
    "\n",
    "def crop_image(image, new_mask):\n",
    "    height, width, channel = np.shape(image)\n",
    "    new_mask = np.asarray(new_mask).astype(\"int\")\n",
    "    new_mask[0] = max(new_mask[0], 0)\n",
    "    new_mask[1] = max(new_mask[1], 0)\n",
    "    new_mask[2] = min(new_mask[2], width)\n",
    "    new_mask[3] = min(new_mask[3], height)\n",
    "    cropped_image = image[new_mask[1]:new_mask[3], new_mask[0]:new_mask[2]]\n",
    "    new_height, new_width, new_channel = np.shape(cropped_image)\n",
    "\n",
    "    if new_height == 0 or new_width == 0:\n",
    "        cropped_image = np.zeros((224, 224, 3))\n",
    "    else:\n",
    "        cv2.resize(cropped_image, (224, 224))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def experience_replay(deep_q_model, experience):\n",
    "    sample = random.choices(experience, k=experience_sample_size)\n",
    "\n",
    "    targets = np.zeros((experience_sample_size, action_option))\n",
    "\n",
    "    for i in range(experience_sample_size):\n",
    "        feature, action, new_feature, reward, end = sample[i]\n",
    "        target = reward\n",
    "\n",
    "        if not end:\n",
    "            target = compute_target(reward, new_feature, deep_q_model)\n",
    "\n",
    "        targets[i, :] = compute_q(feature, deep_q_model)\n",
    "        targets[i][action] = target\n",
    "\n",
    "    x = np.concatenate([each[0] for each in sample])\n",
    "\n",
    "    global loss_arr\n",
    "    loss = deep_q_model.train_on_batch(x, targets)\n",
    "    loss_arr.append(loss)\n",
    "    if len(loss_arr) == 100:\n",
    "        print(\"loss %s\" % str(sum(loss_arr) / len(loss_arr)))\n",
    "        loss_arr = []\n",
    "\n",
    "\n",
    "def train_deep_q(training_epoch, epsilon, image_list, bounding_box_list, deep_q_model, vgg16):\n",
    "    experience = []\n",
    "\n",
    "    for current_epoch in range(1, training_epoch + 1):\n",
    "\n",
    "        print(\"Now starting epoch %d\" % current_epoch)\n",
    "        training_set_size = np.shape(image_list)[0]\n",
    "\n",
    "        for i in range(1000):\n",
    "            image = image_list[i]\n",
    "            ground_truth_box = bounding_box_list[i]\n",
    "            history = [-1] * history_size\n",
    "            height, width, channel = np.shape(image)\n",
    "            current_mask = np.asarray([0, 0, width, height])\n",
    "            feature = extract_feature(image, history, vgg16)\n",
    "            end = False\n",
    "            step = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            while not end:\n",
    "                q_value = compute_q(feature, deep_q_model)\n",
    "                action = select_action(feature, ground_truth_box, step, q_value, epsilon, current_mask)\n",
    "                new_mask, reward, end, history = execute_action(action, history, ground_truth_box, current_mask)\n",
    "                cropped_image = crop_image(image, new_mask)\n",
    "                new_feature = extract_feature(cropped_image, history, vgg16)\n",
    "                if len(experience) > max_experience_size:\n",
    "                    experience = experience[1:]\n",
    "                    experience.append([feature, action, new_feature, reward, end])\n",
    "                else:\n",
    "                    experience.append([feature, action, new_feature, reward, end])\n",
    "\n",
    "                experience_replay(deep_q_model, experience)\n",
    "                feature = new_feature\n",
    "                current_mask = new_mask\n",
    "                step += 1\n",
    "                total_reward += reward\n",
    "\n",
    "            print(\"Image %d, total reward %i\" % (i, total_reward))\n",
    "\n",
    "        if current_epoch < epsilon_change_steps:\n",
    "            epsilon -= 0.1\n",
    "            print(\"current epsilon is %f\" % epsilon)\n",
    "\n",
    "        tf.keras.models.save_model(deep_q_model, \"my_tmp_model.h5\")\n",
    "\n",
    "    return deep_q_model\n",
    "\n",
    "\n",
    "HUBER_DELTA = 1.0\n",
    "def smoothL1(y_true, y_pred):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    x = tf.where(x < HUBER_DELTA, 0.5 * x ** 2, HUBER_DELTA * (x - 0.5 * HUBER_DELTA))\n",
    "    return K.sum(x)\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(4096 + action_option*history_size,), activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(9, activation='linear'))\n",
    "    model.compile(loss=smoothL1, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vgg16():\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=True , pooling='max')\n",
    "    #vgg16.summary()\n",
    "    return vgg16\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # object_number = int(sys.argv[1])\n",
    "    # training_epoch = sys.argv[2]\n",
    "    # epsilon = sys.argv[3]\n",
    "    training_epoch = 10\n",
    "    epsilon = 1\n",
    "    image_list = []\n",
    "    bounding_box_list = []\n",
    "    for i in range(1,21):\n",
    "        image_data, bounding_box_data = load_data(i, test=False)\n",
    "        image_list.append(image_data)\n",
    "        bounding_box_list.append(bounding_box_data)\n",
    "    deep_q_model = create_q_model()\n",
    "    vgg16 = create_vgg16()\n",
    "    trained_model = train_deep_q(training_epoch, epsilon, image_list, bounding_box_list, deep_q_model, vgg16)\n",
    "    trained_model.save(\"model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history_size = 10\n",
    "#action_option = 9\n",
    "#max_steps = 20\n",
    "#experience_sample_size = 20\n",
    "#max_experience_size = 1000\n",
    "#gamma = 0.1\n",
    "#epsilon_change_steps = 10\n",
    "loss_arr = []\n",
    "def test2(iou_threshold):\n",
    "    image_list, bounding_box_list = load_data(test=True)\n",
    "    iou = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    vgg16 = create_vgg16()\n",
    "    deep_q = create_q_model()\n",
    "    deep_q.load_weights(\"model.h5\")\n",
    "    all_predicted_boxes = []\n",
    "    all_groundtruth_boxes = []\n",
    "\n",
    "    for i in range(101):\n",
    "        bounding_box = bounding_box_list[i]\n",
    "        image = image_list[i]\n",
    "        history = [-1] * history_size\n",
    "        height, width, channel = np.shape(image)\n",
    "        current_mask = np.asarray([0, 0, width, height])\n",
    "        feature = extract_feature(image, history, vgg16)\n",
    "        end = False\n",
    "        masks = []\n",
    "        predicted_boxes = []\n",
    "\n",
    "        for step in range(40):  # fixed number of steps\n",
    "            q_value = compute_q(feature, deep_q)\n",
    "            action = np.argmax(q_value)\n",
    "            history = history[1:]\n",
    "            history.append(action)\n",
    "\n",
    "            if action == 8 or step == 39:  # fixed number of steps\n",
    "                end = True\n",
    "                predicted_box = current_mask.tolist()\n",
    "                predicted_boxes.append(predicted_box)\n",
    "                all_predicted_boxes.append(predicted_box)\n",
    "                break\n",
    "            else:\n",
    "                new_mask = compute_mask(action, current_mask)\n",
    "\n",
    "                cropped_image = crop_image(image, new_mask)\n",
    "                feature = extract_feature(cropped_image, history, vgg16)\n",
    "                masks.append(new_mask)\n",
    "                predicted_box = new_mask.tolist()\n",
    "                predicted_boxes.append(predicted_box)\n",
    "                current_mask = new_mask\n",
    "\n",
    "        if not end:\n",
    "            predicted_box = current_mask.tolist()\n",
    "            predicted_boxes.append(predicted_box)\n",
    "            all_predicted_boxes.append(predicted_box)\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for predicted_box in predicted_boxes:\n",
    "            iou_scores = [compute_iou(predicted_box, gt_box) for gt_box in bounding_box_list]\n",
    "            max_iou = max(iou_scores)\n",
    "            if max_iou >= iou_threshold:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        fn = len(bounding_box_list) - tp\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        iou.append(max_iou)\n",
    "\n",
    "    # Calculate AP for each class\n",
    "    ap_list = []\n",
    "    iou_thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "    for iou_thresh in iou_thresholds:\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        total_groundtruth_boxes = len(bounding_box_list)\n",
    "\n",
    "        for i in range(len(all_predicted_boxes)):\n",
    "            iou_score = compute_iou(all_predicted_boxes[i], bounding_box_list[i])\n",
    "            if iou_score >= iou_thresh:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "        if total_groundtruth_boxes == 0:\n",
    "            average_precision = 0\n",
    "        else:\n",
    "            average_precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "        ap_list.append(average_precision)\n",
    "\n",
    "    mAP = np.mean(ap_list)\n",
    "    return mAP\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
